{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMLLk+aihP2Y+r8ZR7cRt+S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NeSma237/GPT-2-Prompt-Engineering/blob/main/GPT_2_Prompt_Engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Fobklyyf6RX",
        "outputId": "f0803723-049a-490d-8210-e7277b8dbf1d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D(nf=2304, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=768)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D(nf=3072, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=3072)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "\n",
        "# Specify the model name\n",
        "model_name = \"gpt2\"\n",
        "\n",
        "# Load the tokenizer and model from Hugging Face\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# Use GPU if available, otherwise fallback to CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2  Design Your Prompts\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "1. Direct instruction:\n",
        "\"Write a motivational quote about overcoming fear.\"\n",
        "\n",
        "2. Scenario-based:  \n",
        "\"Imagine you’re helping a friend who failed a test. Write something encouraging.\"\n",
        "\n",
        "3. Persona-based:  \n",
        "\"As a wise monk, write a quote about inner strength.\"\n",
        "\n",
        "4. Keyword-based:  \n",
        "\"Using the words 'growth', 'struggle', and 'hope', write something inspiring.\"\n",
        "\n",
        "5. Conversational:  \n",
        "\"User: I feel like giving up. GPT-2: Here's a quote for you:\"\n"
      ],
      "metadata": {
        "id": "Emo_a8aNm39o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = {\n",
        "    \"Direct instruction\": \"Write a motivational quote about overcoming fear.\",\n",
        "    \"Scenario-based\": \"Imagine you’re helping a friend who failed a test. Write something encouraging.\",\n",
        "    \"Persona-based\": \"As a wise monk, write a quote about inner strength.\",\n",
        "    \"Keyword-based\": \"Using the words 'growth', 'struggle', and 'hope', write something inspiring.\",\n",
        "    \"Conversational\": \"User: I feel like giving up. GPT-2: Here's a quote for you:\"\n",
        "}\n"
      ],
      "metadata": {
        "id": "5jQZh9FBhfp8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_multiple_outputs(prompt, num_outputs=3, max_length=50):\n",
        "    outputs = []\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
        "    for _ in range(num_outputs):\n",
        "       output = model.generate(\n",
        "        input_ids,\n",
        "        max_length=max_length,\n",
        "        do_sample=True,\n",
        "        temperature=0.8,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        num_return_sequences=1,\n",
        "        pad_token_id=tokenizer.eos_token_id  # Set the padding token ID\n",
        ")\n",
        "\n",
        "       generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "       outputs.append(generated_text)\n",
        "    return outputs\n"
      ],
      "metadata": {
        "id": "UQU5EBothwjc"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for style, prompt in prompts.items():\n",
        "    print(f\"\\n--- {style} Prompt ---\")\n",
        "    outputs = generate_multiple_outputs(prompt)\n",
        "    for i, text in enumerate(outputs, 1):\n",
        "        print(f\"Output {i}:\\n{text}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kBamD1iJhz9f",
        "outputId": "6af8bdfb-f6af-404d-d059-1b2d5f065c5b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Direct instruction Prompt ---\n",
            "Output 1:\n",
            "Write a motivational quote about overcoming fear. This may help you become more productive.\n",
            "\n",
            "Try to make a few changes to your routine.\n",
            "\n",
            "Try to keep your diet low, and at the same time, give yourself a break.\n",
            "\n",
            "\n",
            "\n",
            "Output 2:\n",
            "Write a motivational quote about overcoming fear.\n",
            "\n",
            "6. Talk about how you learned to love yourself.\n",
            "\n",
            "7. Show a lot of love.\n",
            "\n",
            "8. Tell your kids that you love them.\n",
            "\n",
            "9. Don't be\n",
            "\n",
            "Output 3:\n",
            "Write a motivational quote about overcoming fear. You can also add a note about how you will deal with the pain and fear from this problem.\n",
            "\n",
            "2.\n",
            "\n",
            "You'll begin with a short statement about how you feel.\n",
            "\n",
            "What do\n",
            "\n",
            "\n",
            "--- Scenario-based Prompt ---\n",
            "Output 1:\n",
            "Imagine you’re helping a friend who failed a test. Write something encouraging. And when you're not helping someone else, you'll probably find yourself wondering, \"Why am I telling you this?\"\n",
            "\n",
            "5. Don't let your emotions\n",
            "\n",
            "Output 2:\n",
            "Imagine you’re helping a friend who failed a test. Write something encouraging. Take a deep breath. Don't give up. Take a big hard look. If your friend is struggling to get through his test, go to your doctor.\n",
            "\n",
            "\n",
            "Output 3:\n",
            "Imagine you’re helping a friend who failed a test. Write something encouraging. If someone makes you feel better about yourself, please think about it. If you feel depressed, want to help, or want to make friends, try something new.\n",
            "\n",
            "\n",
            "--- Persona-based Prompt ---\n",
            "Output 1:\n",
            "As a wise monk, write a quote about inner strength. I would like to know if you feel the same way.\n",
            "\n",
            "\n",
            "So, what can we do to increase our personal ability to do so?\n",
            "\n",
            "\n",
            "1. Make sure we understand our\n",
            "\n",
            "Output 2:\n",
            "As a wise monk, write a quote about inner strength.\n",
            "\n",
            "In the book, you will also learn to think as well as you think, and to become more like your body. The best part about learning to write a quote is that you\n",
            "\n",
            "Output 3:\n",
            "As a wise monk, write a quote about inner strength. Then, you can tell me how to improve your life.\n",
            "\n",
            "What's the most difficult thing you have ever done?\n",
            "\n",
            "I'm not a bad person, but I'm not\n",
            "\n",
            "\n",
            "--- Keyword-based Prompt ---\n",
            "Output 1:\n",
            "Using the words 'growth', 'struggle', and 'hope', write something inspiring. The hope is to build something that will make it happen. The hope is to make it possible to overcome our difficulties and turn away from the struggle. This\n",
            "\n",
            "Output 2:\n",
            "Using the words 'growth', 'struggle', and 'hope', write something inspiring. A successful and successful person needs to have a deep understanding of what they have just experienced.\n",
            "\n",
            "\n",
            "When you've experienced success, it's time to start\n",
            "\n",
            "Output 3:\n",
            "Using the words 'growth', 'struggle', and 'hope', write something inspiring.\n",
            "\n",
            "\"The story of my life begins with the work I put into teaching my children about the world, and how they can learn to value it,\n",
            "\n",
            "\n",
            "--- Conversational Prompt ---\n",
            "Output 1:\n",
            "User: I feel like giving up. GPT-2: Here's a quote for you: \"If you want to participate in a group (and you want to do it on your own), we're going to give you that!\" You can\n",
            "\n",
            "Output 2:\n",
            "User: I feel like giving up. GPT-2: Here's a quote for you:\n",
            "\n",
            "\"This is not about getting you back to the game.\"\n",
            "\n",
            "\"No, I'm not going to give up on it.\"\n",
            "\n",
            "\n",
            "Output 3:\n",
            "User: I feel like giving up. GPT-2: Here's a quote for you:\n",
            "\n",
            "\"It's your right to be there as long as you want,\" said Ralston. \"But it's also your right to be\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3 – Human-Written References\n",
        "\n",
        "### 1. Direct Instruction Prompt\n",
        "**Prompt:** Write a motivational quote about overcoming fear.  \n",
        "**Human Reference:** “Everything you’ve ever wanted is on the other side of fear.” – George Addair  \n",
        "**Source:** https://www.goodreads.com/quotes/2275\n",
        "\n",
        "### 2. Scenario-Based Prompt\n",
        "**Prompt:** Imagine you’re helping a friend who failed a test. Write something encouraging.  \n",
        "**Human Reference:** “Failure is simply the opportunity to begin again, this time more intelligently.” – Henry Ford  \n",
        "**Source:** https://www.brainyquote.com/quotes/henry_ford_131621\n",
        "\n",
        "### 3. Persona-Based Prompt\n",
        "**Prompt:** As a wise monk, write a quote about inner strength.  \n",
        "**Human Reference:** “You have power over your mind — not outside events. Realize this, and you will find strength.” – Marcus Aurelius  \n",
        "**Source:** https://www.goodreads.com/quotes/12478\n",
        "\n",
        "### 4. Keyword-Based Prompt\n",
        "**Prompt:** Using the words 'growth', 'struggle', and 'hope', write something inspiring.  \n",
        "**Human Reference:** “Out of difficulties grow miracles.” – Jean de La Bruyère  \n",
        "**Source:** https://www.goodreads.com/quotes/6867\n",
        "\n",
        "### 5. Conversational Prompt\n",
        "**Prompt:** User: I feel like giving up. GPT-2: Here's a quote for you:  \n",
        "**Human Reference:** “When you feel like giving up, remember why you held on for so long in the first place.” – Unknown  \n",
        "**Source:** https://www.keepinspiring.me/quotes-about-not-giving-up/\n"
      ],
      "metadata": {
        "id": "du9A_xOpj5m1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bert_score\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6T-BkwPMjXma",
        "outputId": "72e96c90-4171-4391-8ef3-5de0e149fc21"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bert_score\n",
            "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.6.0+cu124)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.2.2)\n",
            "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert_score) (4.51.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.11/dist-packages (from bert_score) (4.67.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert_score) (3.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bert_score) (24.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2025.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.0.0->bert_score)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.0.0->bert_score)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.0.0->bert_score)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.0.0->bert_score)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.0.0->bert_score)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.0.0->bert_score)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.0.0->bert_score)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.0.0->bert_score)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.0.0->bert_score)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.0.0->bert_score)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert_score) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.31.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.5.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (3.2.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (2025.4.26)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert_score) (3.0.2)\n",
            "Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m103.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bert_score\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed bert_score-0.3.13 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nvidia"
                ]
              },
              "id": "bd63c81236994518af3866e535a1eff5"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install bert_score\n",
        "\n",
        "from bert_score import score\n",
        "\n",
        "human_references = {\n",
        "    \"Direct instruction\": \"Overcoming fear is the first step to success and growth.\",\n",
        "    \"Scenario-based\": \"Failing is not the end; it's a chance to try again smarter and stronger.\",\n",
        "    \"Persona-based\": \"True inner strength comes from understanding and accepting yourself.\",\n",
        "    \"Keyword-based\": \"Growth often comes through struggle, but hope keeps us moving forward.\",\n",
        "    \"Conversational\": \"Don't give up; every challenge is a step toward your better self.\"\n",
        "}\n",
        "\n",
        "all_outputs = {\n",
        "    \"Direct instruction\": [\n",
        "        \"Write a motivational quote about overcoming fear. This may help you become more productive.\\nTry to make a few changes to your routine.\\nTry to keep your diet low, and at the same time, give yourself a break.\",\n",
        "        \"Write a motivational quote about overcoming fear.\\n6. Talk about how you learned to love yourself.\\n7. Show a lot of love.\\n8. Tell your kids that you love them.\\n9. Don't be\",\n",
        "        \"Write a motivational quote about overcoming fear. You can also add a note about how you will deal with the pain and fear from this problem.\\n2.\\nYou'll begin with a short statement about how you feel.\\nWhat do\"\n",
        "    ],\n",
        "    \"Scenario-based\": [\n",
        "        \"Imagine you’re helping a friend who failed a test. Write something encouraging. And when you're not helping someone else, you'll probably find yourself wondering, \\\"Why am I telling you this?\\\"\\n5. Don't let your emotions\",\n",
        "        \"Imagine you’re helping a friend who failed a test. Write something encouraging. Take a deep breath. Don't give up. Take a big hard look. If your friend is struggling to get through his test, go to your doctor.\",\n",
        "        \"Imagine you’re helping a friend who failed a test. Write something encouraging. If someone makes you feel better about yourself, please think about it. If you feel depressed, want to help, or want to make friends, try something new.\"\n",
        "    ],\n",
        "    \"Persona-based\": [\n",
        "        \"As a wise monk, write a quote about inner strength. I would like to know if you feel the same way.\\nSo, what can we do to increase our personal ability to do so?\\n1. Make sure we understand our\",\n",
        "        \"As a wise monk, write a quote about inner strength.\\nIn the book, you will also learn to think as well as you think, and to become more like your body. The best part about learning to write a quote is that you\",\n",
        "        \"As a wise monk, write a quote about inner strength. Then, you can tell me how to improve your life.\\nWhat's the most difficult thing you have ever done?\\nI'm not a bad person, but I'm not\"\n",
        "    ],\n",
        "    \"Keyword-based\": [\n",
        "        \"Using the words 'growth', 'struggle', and 'hope', write something inspiring. The hope is to build something that will make it happen. The hope is to make it possible to overcome our difficulties and turn away from the struggle. This\",\n",
        "        \"Using the words 'growth', 'struggle', and 'hope', write something inspiring. A successful and successful person needs to have a deep understanding of what they have just experienced.\\nWhen you've experienced success, it's time to start\",\n",
        "        \"Using the words 'growth', 'struggle', and 'hope', write something inspiring.\\n\\\"The story of my life begins with the work I put into teaching my children about the world, and how they can learn to value it,\"\n",
        "    ],\n",
        "    \"Conversational\": [\n",
        "        \"User: I feel like giving up. GPT-2: Here's a quote for you: \\\"If you want to participate in a group (and you want to do it on your own), we're going to give you that!\\\" You can\",\n",
        "        \"User: I feel like giving up. GPT-2: Here's a quote for you:\\n\\\"This is not about getting you back to the game.\\\"\\n\\\"No, I'm not going to give up on it.\\\"\",\n",
        "        \"User: I feel like giving up. GPT-2: Here's a quote for you:\\n\\\"It's your right to be there as long as you want,\\\" said Ralston. \\\"But it's also your right to be\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "for prompt_type, outputs in all_outputs.items():\n",
        "    refs = [human_references[prompt_type]] * len(outputs)\n",
        "    P, R, F1 = score(outputs, refs, lang=\"en\", verbose=False)\n",
        "    print(f\"\\n--- {prompt_type} ---\")\n",
        "    for i, (p, r, f1) in enumerate(zip(P, R, F1), 1):\n",
        "        print(f\"Output {i}: Precision={p:.4f}, Recall={r:.4f}, F1={f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJiiDJvtlxMm",
        "outputId": "a637eea7-0791-4e6e-c1c2-b4014865bc74"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Direct instruction ---\n",
            "Output 1: Precision=0.8721, Recall=0.8985, F1=0.8851\n",
            "Output 2: Precision=0.8618, Recall=0.8773, F1=0.8695\n",
            "Output 3: Precision=0.8552, Recall=0.8903, F1=0.8724\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Scenario-based ---\n",
            "Output 1: Precision=0.8393, Recall=0.8522, F1=0.8457\n",
            "Output 2: Precision=0.8503, Recall=0.8600, F1=0.8551\n",
            "Output 3: Precision=0.8486, Recall=0.8562, F1=0.8524\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Persona-based ---\n",
            "Output 1: Precision=0.8402, Recall=0.8906, F1=0.8647\n",
            "Output 2: Precision=0.8370, Recall=0.8850, F1=0.8603\n",
            "Output 3: Precision=0.8302, Recall=0.8860, F1=0.8572\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Keyword-based ---\n",
            "Output 1: Precision=0.8367, Recall=0.8688, F1=0.8525\n",
            "Output 2: Precision=0.8323, Recall=0.8681, F1=0.8498\n",
            "Output 3: Precision=0.8315, Recall=0.8737, F1=0.8521\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Conversational ---\n",
            "Output 1: Precision=0.8481, Recall=0.8823, F1=0.8649\n",
            "Output 2: Precision=0.8473, Recall=0.8847, F1=0.8656\n",
            "Output 3: Precision=0.8285, Recall=0.8675, F1=0.8475\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from bert_score import score\n",
        "\n",
        "human_references = {\n",
        "    \"Direct instruction\": \"Overcoming fear is the first step to success and growth.\",\n",
        "    \"Scenario-based\": \"Failing is not the end; it's a chance to try again smarter and stronger.\",\n",
        "    \"Persona-based\": \"True inner strength comes from understanding and accepting yourself.\",\n",
        "    \"Keyword-based\": \"Growth often comes through struggle, but hope keeps us moving forward.\",\n",
        "    \"Conversational\": \"Don't give up; every challenge is a step toward your better self.\"\n",
        "}\n",
        "\n",
        "all_outputs = {\n",
        "    \"Direct instruction\": [\n",
        "        \"Write a motivational quote about overcoming fear. This may help you become more productive.\\nTry to make a few changes to your routine.\\nTry to keep your diet low, and at the same time, give yourself a break.\",\n",
        "        \"Write a motivational quote about overcoming fear.\\n6. Talk about how you learned to love yourself.\\n7. Show a lot of love.\\n8. Tell your kids that you love them.\\n9. Don't be\",\n",
        "        \"Write a motivational quote about overcoming fear. You can also add a note about how you will deal with the pain and fear from this problem.\\n2.\\nYou'll begin with a short statement about how you feel.\\nWhat do\"\n",
        "    ],\n",
        "    \"Scenario-based\": [\n",
        "        \"Imagine you’re helping a friend who failed a test. Write something encouraging. And when you're not helping someone else, you'll probably find yourself wondering, \\\"Why am I telling you this?\\\"\\n5. Don't let your emotions\",\n",
        "        \"Imagine you’re helping a friend who failed a test. Write something encouraging. Take a deep breath. Don't give up. Take a big hard look. If your friend is struggling to get through his test, go to your doctor.\",\n",
        "        \"Imagine you’re helping a friend who failed a test. Write something encouraging. If someone makes you feel better about yourself, please think about it. If you feel depressed, want to help, or want to make friends, try something new.\"\n",
        "    ],\n",
        "    \"Persona-based\": [\n",
        "        \"As a wise monk, write a quote about inner strength. I would like to know if you feel the same way.\\nSo, what can we do to increase our personal ability to do so?\\n1. Make sure we understand our\",\n",
        "        \"As a wise monk, write a quote about inner strength.\\nIn the book, you will also learn to think as well as you think, and to become more like your body. The best part about learning to write a quote is that you\",\n",
        "        \"As a wise monk, write a quote about inner strength. Then, you can tell me how to improve your life.\\nWhat's the most difficult thing you have ever done?\\nI'm not a bad person, but I'm not\"\n",
        "    ],\n",
        "    \"Keyword-based\": [\n",
        "        \"Using the words 'growth', 'struggle', and 'hope', write something inspiring. The hope is to build something that will make it happen. The hope is to make it possible to overcome our difficulties and turn away from the struggle. This\",\n",
        "        \"Using the words 'growth', 'struggle', and 'hope', write something inspiring. A successful and successful person needs to have a deep understanding of what they have just experienced.\\nWhen you've experienced success, it's time to start\",\n",
        "        \"Using the words 'growth', 'struggle', and 'hope', write something inspiring.\\n\\\"The story of my life begins with the work I put into teaching my children about the world, and how they can learn to value it,\"\n",
        "    ],\n",
        "    \"Conversational\": [\n",
        "        \"User: I feel like giving up. GPT-2: Here's a quote for you: \\\"If you want to participate in a group (and you want to do it on your own), we're going to give you that!\\\" You can\",\n",
        "        \"User: I feel like giving up. GPT-2: Here's a quote for you:\\n\\\"This is not about getting you back to the game.\\\"\\n\\\"No, I'm not going to give up on it.\\\"\",\n",
        "        \"User: I feel like giving up. GPT-2: Here's a quote for you:\\n\\\"It's your right to be there as long as you want,\\\" said Ralston. \\\"But it's also your right to be\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Prepare data list for the DataFrame\n",
        "data = []\n",
        "\n",
        "for prompt_type, outputs in all_outputs.items():\n",
        "    references = [human_references[prompt_type]] * len(outputs)\n",
        "    P, R, F1 = score(outputs, references, lang=\"en\", verbose=False)\n",
        "    for i, f1 in enumerate(F1, start=1):\n",
        "        data.append({\n",
        "            \"Prompt Type\": prompt_type,\n",
        "            \"Output #\": i,\n",
        "            \"BERTScore F1\": f1.item()\n",
        "        })\n",
        "\n",
        "# Create DataFrame and display\n",
        "df_results = pd.DataFrame(data)\n",
        "print(df_results)\n",
        "\n",
        "# Optionally save to CSV\n",
        "df_results.to_csv(\"bert_score_results.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVAY560smSfP",
        "outputId": "5b9194c0-3096-406f-9950-8e8bf01f351d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           Prompt Type  Output #  BERTScore F1\n",
            "0   Direct instruction         1      0.885096\n",
            "1   Direct instruction         2      0.869458\n",
            "2   Direct instruction         3      0.872385\n",
            "3       Scenario-based         1      0.845679\n",
            "4       Scenario-based         2      0.855129\n",
            "5       Scenario-based         3      0.852401\n",
            "6        Persona-based         1      0.864670\n",
            "7        Persona-based         2      0.860307\n",
            "8        Persona-based         3      0.857188\n",
            "9        Keyword-based         1      0.852462\n",
            "10       Keyword-based         2      0.849810\n",
            "11       Keyword-based         3      0.852110\n",
            "12      Conversational         1      0.864858\n",
            "13      Conversational         2      0.865580\n",
            "14      Conversational         3      0.847516\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Analysis:**\n",
        "\n",
        "The highest BERTScore was achieved by the first output of the Direct Instruction prompt (0.8851), indicating it has the greatest semantic similarity to the human-written motivational quote. Overall, all prompts generated outputs with relatively high semantic similarity scores (above 0.84), showing that GPT-2 can produce motivational text closely aligned with the reference across diverse prompt styles."
      ],
      "metadata": {
        "id": "mfUglwnnwzAp"
      }
    }
  ]
}